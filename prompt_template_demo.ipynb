{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c590dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the content in 50 words or fewer:\n",
      "\n",
      "The ability to learn from raw text without manual labeling is crucial for natural language processing (NLP). This paper explores a semi-supervised approach combining unsupervised pre-training and supervised fine-tuning to develop a universal representation that transfers well across various NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\", \"max_words\"],\n",
    "    template=\"\"\"\n",
    "    Please summarize the following content in {max_words} words or fewer.\n",
    "    Focus on the main points and key takeaways.\n",
    "\n",
    "    Content: {content}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "content = \"\"\"\n",
    "The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised\n",
    "learning in natural language processing (NLP). Most deep learning methods require substantial\n",
    "amounts of manually labeled data, which restricts their applicability in many domains that suffer\n",
    "from a dearth of annotated resources. In these situations, models that can leverage linguistic\n",
    "information from unlabeled data provide a valuable alternative to gathering more annotation, which\n",
    "can be time-consuming and expensive. Further, even in cases where considerable supervision\n",
    "is available, learning good representations in an unsupervised fashion can provide a significant\n",
    "performance boost. The most compelling evidence for this so far has been the extensive use of pre-\n",
    "trained word embeddings to improve performance on a range of NLP tasks.\n",
    "\n",
    "Leveraging more than word-level information from unlabeled text, however, is challenging for two\n",
    "main reasons. First, it is unclear what type of optimization objectives are most effective at learning\n",
    "text representations that are useful for transfer. Recent research has looked at various objectives\n",
    "such as language modeling, machine translation, and discourse coherence, with each\n",
    "method outperforming the others on different tasks. Second, there is no consensus on the most\n",
    "effective way to transfer these learned representations to the target task. Existing techniques involve\n",
    "a combination of making task-specific changes to the model architecture, using intricate\n",
    "learning schemes and adding auxiliary learning objectives. These uncertainties have made\n",
    "it difficult to develop effective semi-supervised learning approaches for language processing.\n",
    "\n",
    "In this paper, we explore a semi-supervised approach for language understanding tasks using a\n",
    "combination of unsupervised pre-training and supervised fine-tuning. Our goal is to learn a universal\n",
    "representation that transfers with little adaptation to a wide range of tasks. We assume access to\n",
    "a large corpus of unlabeled text and several datasets with manually annotated training examples\n",
    "(target tasks). Our setup does not require these target tasks to be in the same domain as the unlabeled\n",
    "corpus. We employ a two-stage training procedure. First, we use a language modeling objective on\n",
    "the unlabeled data to learn the initial parameters of a neural network model. Subsequently, we adapt\n",
    "these parameters to a target task using the corresponding supervised objective.\n",
    "\"\"\"\n",
    "\n",
    "formatted_prompt = summary_prompt.format(\n",
    "    content=content,\n",
    "    max_words=\"50\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
